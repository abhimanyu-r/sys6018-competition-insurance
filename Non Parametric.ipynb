{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Approach (Random Forests)\n",
    "\n",
    "Here we explore a python implementation of random forest for this competition. The reason for the transition to python was a computational one. R was proving to be far too slow to create even trivially sized forests. Python gives an increase in speed by nature, but also has more natural integration of parallel tree creation, allowing for the creation of larger forests. That being said, computation is still an issue and will come into play as we go through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Coefficient\n",
    "\n",
    "Code, adapted from the code on collab, to compute the gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalized_gini_index(g, predicted_probabilities):\n",
    "  \n",
    "    if len(g) != len(predicted_probabilities):\n",
    "        print(\"Actual and Predicted need to be equal lengths!\")\n",
    "        return\n",
    "\n",
    "    # arrange data into table with columns of index, predicted values, and actual values\n",
    "    d = {\"truth\": g, \"pred\": predicted_probabilities}\n",
    "    gini_table = pd.DataFrame(data = d, index = range(1,len(g) + 1))\n",
    "\n",
    "    # sort rows in decreasing order of the predicted values, breaking ties according to the index\n",
    "    # gini_table = gini.table[order(-gini.table$predicted.probabilities, gini.table$index), ]\n",
    "    gini_table = gini_table.sort_values(\"pred\", ascending = False)\n",
    "\n",
    "    # get the per-row increment for positives accumulated by the model \n",
    "    num_ground_truth_positivies = sum(gini_table[\"truth\"])\n",
    "    model_percentage_positives_accumulated = gini_table[\"truth\"] / num_ground_truth_positivies\n",
    "\n",
    "    # get the per-row increment for positives accumulated by a random guess\n",
    "    random_guess_percentage_positives_accumulated = 1 / len(gini_table[\"truth\"])\n",
    "\n",
    "    # calculate gini index\n",
    "    gini_sum = np.cumsum(model_percentage_positives_accumulated - random_guess_percentage_positives_accumulated)\n",
    "    gini_index = sum(gini_sum) / len(gini_table[\"truth\"]) \n",
    "    return(gini_index)\n",
    "\n",
    "\n",
    "#' Calculates normalized Gini index from ground truth and predicted probabilities.\n",
    "#' @param ground.truth Ground-truth scalar values (e.g., 0 and 1)\n",
    "#' @param predicted.probabilities Predicted probabilities for the items listed in ground.truth\n",
    "#' @return Normalized Gini index, accounting for theoretical optimal.\n",
    "def normalized_gini_index(g, predicted_probabilities):\n",
    "    model_gini_index = unnormalized_gini_index(g, predicted_probabilities)\n",
    "    optimal_gini_index = unnormalized_gini_index(g, g)\n",
    "    return(model_gini_index / optimal_gini_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train[\"target\"]\n",
    "x = train.drop([\"id\", \"target\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to just try and fit a random forest to the raw train data and get a baseline for the gini index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=2, random_state=0, n_jobs=-1, max_features=\"log2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "ids = test[\"id\"]\n",
    "test = test.drop([\"id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(test)\n",
    "probs_final = [x[1] for x in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 = {'id':ids, 'target':probs_final}\n",
    "result1_df = pd.DataFrame(data = result1)\n",
    "\n",
    "result1_df.to_csv(\"predictionsRF10-24-17.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gave a baseline score of  0.232\n",
    "\n",
    "Now we transition to our actual approach. We will use a 3 fold cross validation approach at first. We have to select a small number of folds because it is computational infeasible to do this with a larger number of folds. We also want to collect which variables are the most important. The way we will approach this, is that for each of the trees generated in cross fold, we will store which variables are in the top 20 most important variables for that tree. 20 was chosen somewhat arbitrarily, but it was a value at which the importance seemed to drop off a bit. We will then see which variables show up as important in all folds and proceed with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "y_kfold = np.array(train[\"target\"])\n",
    "x_kfold = np.array(train.drop([\"id\", \"target\"], axis = 1))\n",
    "kf = StratifiedKFold(n_splits=3)\n",
    "kf.get_n_splits(x_kfold, y_kfold)\n",
    "forest = ExtraTreesClassifier(n_estimators=500, max_depth=2, random_state=0, n_jobs=-1, max_features=\"log2\")\n",
    "\n",
    "ginis = []\n",
    "\n",
    "ratio = [0]*57\n",
    "\n",
    "for train_index, test_index in kf.split(x_kfold, y_kfold):\n",
    "    X_train, X_test = x_kfold[train_index], x_kfold[test_index]\n",
    "    y_train, y_test = y_kfold[train_index], y_kfold[test_index]\n",
    "    \n",
    "    model = forest.fit(X_train, y_train)\n",
    "    probs = forest.predict_proba(X_test)\n",
    "    probs_final = [x[1] for x in probs]\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    index = 0\n",
    "    for f in range(X_train.shape[1]):\n",
    "        # print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "        if(index < 20):\n",
    "            ratio[indices[f]] += 1\n",
    "        index += 1\n",
    "\n",
    "    ginis.append(normalized_gini_index(g=y_test, predicted_probabilities=probs_final))\n",
    "    print(len(ginis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21582272536451991, 0.22396413793473299, 0.22061223708988106]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 34]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the cross validation Gini\n",
    "print(ginis)\n",
    "\n",
    "# Find which variables top level of important in all folds\n",
    "interest = [x for x in range(0,57) if ratio[x] > 2]\n",
    "interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives Gini scores of [0.21581968897515907, 0.22396513332529083, 0.22062593969640684]\n",
    "and important columns of [3, 4, 5, 6, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now redo but this time use only the most important predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "y_kfold = np.array(train[\"target\"])\n",
    "x_kfold = train.drop([\"id\", \"target\"], axis = 1)\n",
    "x_kfold = np.array(x_kfold.iloc[:, interest])\n",
    "kf = StratifiedKFold(n_splits=3)\n",
    "kf.get_n_splits(x_kfold, y_kfold)\n",
    "forest = RandomForestClassifier(n_estimators=500, max_depth=2, random_state=0, n_jobs=-1)\n",
    "\n",
    "ginis = []\n",
    "\n",
    "for train_index, test_index in kf.split(x_kfold, y_kfold):\n",
    "    X_train, X_test = x_kfold[train_index], x_kfold[test_index]\n",
    "    y_train, y_test = y_kfold[train_index], y_kfold[test_index]\n",
    "    \n",
    "    model = forest.fit(X_train, y_train)\n",
    "    probs = forest.predict_proba(X_test)\n",
    "    probs_final = [x[1] for x in probs]\n",
    "    \n",
    "    ginis.append(normalized_gini_index(g=y_test, predicted_probabilities=probs_final))\n",
    "    print(len(ginis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22515354264447354, 0.23158850697725489, 0.22609265323812991]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ginis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new Ginis are [0.22515354264447354, 0.23158850697725489, 0.22609265323812991]\n",
    "The reduction in features didn't cause the Gini's to drop at all (in fact, they went up a bit), but the computation time also did not drop at all. The next step is to use our reduced model and try to up the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "y_kfold = np.array(train[\"target\"])\n",
    "x_kfold = train.drop([\"id\", \"target\"], axis = 1)\n",
    "x_kfold = np.array(x_kfold.iloc[:, interest])\n",
    "kf = StratifiedKFold(n_splits=3)\n",
    "kf.get_n_splits(x_kfold, y_kfold)\n",
    "forest = RandomForestClassifier(n_estimators=500, n_jobs=-1, max_depth=11)\n",
    "\n",
    "ginis = []\n",
    "\n",
    "for train_index, test_index in kf.split(x_kfold, y_kfold):\n",
    "    X_train, X_test = x_kfold[train_index], x_kfold[test_index]\n",
    "    y_train, y_test = y_kfold[train_index], y_kfold[test_index]\n",
    "    \n",
    "    forest.fit(X_train, y_train)\n",
    "    probs = forest.predict_proba(X_test)\n",
    "    probs_final = [x[1] for x in probs]\n",
    "    \n",
    "    ginis.append(normalized_gini_index(g=y_test, predicted_probabilities=probs_final))\n",
    "    print(len(ginis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ginis for various tree sizes, 3 fold, max depth 2:\n",
    "\n",
    "n = 500  : [0.22450751789755535, 0.23204761929794665, 0.22267593701940497]\n",
    "\n",
    "n = 1000 : [0.22280302864951337, 0.23167452434680696, 0.22335295226606269]\n",
    "\n",
    "n = 2000 : [0.22311711050854513, 0.23229558294968261, 0.22425416160044812]\n",
    "\n",
    "n = 3000 : [0.22302677539348212, 0.2321572048538286, 0.22492528983348847]\n",
    "\n",
    "Tree size seems to have little effects, now we look at the results of increasing depth, n = 500, 3 fold\n",
    "\n",
    "max depth = 3 : [0.2323206135305306, 0.23691938332400231, 0.23321463325695618]\n",
    "\n",
    "max depth = 4 : [0.2386918741050057, 0.24210880425554068, 0.23738067414812231]\n",
    "\n",
    "max depth = 5 : [0.24352711368719027, 0.24594164530106855, 0.24283873295515726]\n",
    "\n",
    "max depth = 6 : [0.24726379801321008, 0.24851153683176316, 0.24561519123072281]\n",
    "\n",
    "max depth = 7 : [0.24926826777510488, 0.25154541925431562, 0.24873424215860127]\n",
    "\n",
    "max depth = 8 : [0.25186935115884101, 0.25244487181359732, 0.25104524729788208]\n",
    "\n",
    "max depth = 9 : [0.25291066201294671, 0.25376964260539919, 0.25267636240651586]\n",
    "\n",
    "max depth = 10 : [0.25414194176838978, 0.25408899804070506, 0.25366274897182656]\n",
    "\n",
    "max depth = 11 : [0.25389078795774939, 0.25407960693588516, 0.25374236182468013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25389078795774939, 0.25407960693588516, 0.25374236182468013]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ginis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
